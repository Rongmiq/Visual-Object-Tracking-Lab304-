# Visual-Object-Tracking-Lab304

### CV三大顶会论文分类汇总(2020-Now)(@[52cv](https://github.com/52CV))

# Multi-Object Tracking (MOT)
Here is a collection of outstanding MOT work from top conferences and journals. Some of them (such as SORT, DeepSORT, JDE, etc.) are highly influential in the field and attract much attention. The number of ⭐ in the table represents the level of attention it deserves, of course this is just a personal opinion. Hope it can be useful to you！
### OVERVIEW (MOT17 test)
The table shows the results on the MOT17 test set and xx* means unofficial results. TBD, JTD, and Siam are short for Tracking-by-Detection, Joint-Detection-and-Tracking, and Siamese Network respectively. M, R, Tr, and G represent Motion information, ReID feature, Transformer, and Graph Neural Network respectively.
|    Method     | Type | Keywords  |                                                                         Ref@ Paper                                                                         |                                                         PageHome                                                         | MOTA ↑ | IDF1 ↑ | HOTA ↑ | IDs ↓ | Stars  |
|:-------------:|:----:|:---------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------------------------:|:------:|:------:|:------:|:-----:|:------|
|     SORT      | TBD  |     M     |                                                        [ICIP2016](https://arxiv.org/abs/1602.00763)                                                        | [Official](https://github.com/abewley/sort) or [Other](https://github.com/ifzhang/ByteTrack/tree/main/yolox/sort_tracker) |  43.1  |  39.8  |  34.0  | 4852  | ⭐⭐⭐⭐⭐ |
|   DeepSORT    | TBD  |    M+R    |                                                        [ICIP2017](https://arxiv.org/abs/1703.07402)                                                        |                        [Other](https://github.com/ifzhang/ByteTrack/tree/main/yolox/sort_tracker)                        | 78.0*  | 74.5*  | 61.2*  | 1821* | ⭐⭐⭐⭐⭐ |
|    DeepMOT    |  -   |     -     |                                                        [CVPR2020](https://arxiv.org/abs/1906.06618)                                                        |                                  [Official](https://gitlab.inria.fr/robotlearn/deepmot)                                   |  53.7  |  53.8  |  42.4  | 1,947 | ⭐⭐⭐   |
|   Trackor++   | TBD  |    M+R    |                                                        [ICCV2019](https://arxiv.org/abs/1903.05625)                                                        |                               [Official](https://github.com/phil-bergmann/tracking_wo_bnw)                                |  56.3  |  55.1  |  44.8  | 1,987 | ⭐⭐⭐   |
|    TubeTK     | JDT  |     M     |                                                      [CVPR2020](https://arxiv.org/pdf/2006.05683.pdf)                                                      |                                     [Official](https://github.com/BoPang1996/TubeTK)                                      |  63.0  |  58.6  |  48.0  | 4,137 | ⭐⭐    |
|    ArTIST     | TBD  |     M     |                                                        [CVPR2021](https://arxiv.org/abs/2012.02337)                                                        |                                     [Official](https://github.com/fatemeh-slh/ArTIST)                                     |  62.3  |  59.7  |  48.9  | 2,062 | ⭐⭐    |
|      JDE      | JDT  |    M+R    |                                                        [ECCV2020](https://arxiv.org/abs/1909.12605)                                                        |                               [Official](https://github.com/Zhongdao/Towards-Realtime-MOT)                                | 63.2*  | 65.0*  |   -    |   -   | ⭐⭐⭐⭐⭐ |
|   CTracker    | TBD  |   Chain   |                                                        [ECCV2020](https://arxiv.org/abs/2007.14557)                                                        |                                      [Official](https://github.com/pjl1995/CTracker)                                      |  66.6  |  57.4  |  49.0  | 5529  | ⭐⭐⭐   |
|  CenterTrack  | JDT  |     M     |                                                        [ECCV2020](https://arxiv.org/abs/2004.01177)                                                        |                                   [Official](https://github.com/xingyizhou/CenterTrack)                                   |  67.8  |  64.7  |  52.2  | 3,039 | ⭐⭐⭐⭐  |
|    TraDes     | JDT  |    M+R    |                                                      [CVPR2021](https://arxiv.org/pdf/2103.08808.pdf)                                                      |                                      [Official](https://github.com/JialianW/TraDeS)                                       |  69.1  |  63.9  |  52.7  | 3555  | ⭐⭐⭐   |
|    CSTrack    | JDT  |    M+R    |                                                      [IEEE TIP2021](https://arxiv.org/abs/2010.12138)                                                      |                                       [Official](https://github.com/JudasDie/SOTS)                                        |  74.9  |  72.6  |  59.3  | 3567  | ⭐⭐⭐   |
|  TransTrack   | Siam |    Tr     |                                                       [arxiv2021](https://arxiv.org/abs/2012.15460)                                                        |                                    [Official](https://github.com/PeizeSun/TransTrack)                                     |  75.2  |  63.5  |  54.1  | 3,603 | ⭐⭐⭐   |
|    SOTMOT     | Siam |     -     | [CVPR2021](https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Improving_Multiple_Object_Tracking_With_Single_Object_Tracking_CVPR_2021_paper.pdf) |                                                            -                                                             |  71.0  |  71.9  |   -    | 5184  | ⭐⭐    |
|  TransCenter  | JDT  |   M+Tr    |                                                       [arxiv2021](https://arxiv.org/abs/2103.15145)                                                        |                                    [Official](https://github.com/yihongXU/TransCenter)                                    |  73.2  |  62.2  |  54.5  | 4,614 | ⭐⭐    |
|     GSDT      | JDT  |   M+TrG   |                                                        [ICRA2021](https://arxiv.org/abs/2006.13164)                                                        |                                       [Official](https://github.com/yongxinw/GSDT)                                        |  66.2  |  68.7  |  55.5  | 3,318 | ⭐⭐⭐   |
|  PermaTrack   | JDT  |     M     |                                                      [ICCV2021](https://arxiv.org/pdf/2103.14258.pdf)                                                      |                                     [Official](https://github.com/TRI-ML/permatrack)                                      |  73.8  |  68.9  |  55.5  | 3,699 | ⭐⭐    |
|    FairMOT    | JDT  |    M+R    |                                                        [IJCV2021](https://arxiv.org/abs/2004.01888)                                                        |                                      [Official](https://github.com/ifzhang/FairMOT)                                       |  73.7  |  72.3  |  59.3  | 3,303 | ⭐⭐⭐⭐⭐ |
|    SiamMOT    | Siam |     M     |                                 [CVPR2021](https://www.amazon.science/publications/siammot-siamese-multi-object-tracking)                                  |                                  [Official](https://github.com/amazon-research/siam-mot)                                  |  76.3  |  72.3  |   -    |   -   | ⭐⭐⭐   |
|  CorrTracker  | JDT  |     M     |                                                        [CVPR2021](http://arxiv.org/abs/2104.03541)                                                         |                                                            -                                                             |  76.5  |  73.6  |  60.7  | 3,369 | ⭐⭐    |
| RelationTrack | JDT  |  M+R+Tr   |                                                       [arxiv2021](https://arxiv.org/abs/2105.04322)                                                        |                                    [Official](https://github.com/Ahnsun/RelationTrack)                                    |  73.8  |  74.7  |  61.0  | 1,374 | ⭐⭐    |
|   TransMOT    | JDT  | M+R+Tr+G  |                                                       [arxiv2021](https://arxiv.org/abs/2104.00194)                                                        |                                                            -                                                             |  76.7  |  75.1  |  61.7  | 2,346 | ⭐⭐    |
|   ByteTrack   | TBD  |     M     |                                                        [ECCV2022](https://arxiv.org/abs/2110.06864)                                                        |                                     [Official](https://github.com/ifzhang/ByteTrack)                                      |  80.3  |  77.3  |  63.1  | 2,196 | ⭐⭐⭐⭐⭐ |
|    OC_SORT    | TBD  |     M     |                                                       [arxiv2022](https://arxiv.org/abs/2203.14360)                                                        |                                      [Official](https://github.com/noahcao/OC_SORT)                                       |  78.0  |  77.5  |  63.2  | 2,196 | ⭐⭐⭐⭐  |
| StrongSORT++  | TBD  |    M+R    |                                                       [arxiv2022](https://arxiv.org/abs/2202.13514)                                                        |  [Official](https://github.com/dyhBUPT/StrongSORT) or [Other](https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet)  |  79.6  |  79.5  |  64.4  | 1,194 | ⭐⭐⭐⭐  |
|BOT-SORT(ReID) |TBD   |   M+R     |                                                       [arxiv2022](https://arxiv.org/pdf/2206.14651.pdf)                                                    |                                     [Official](https://github.com/NirAharon/BOT-SORT)                                     |80.6    |79.5    |64.6    |   1257|  ⭐⭐⭐  |

# Datasets
We provide several relevant datasets for training and evaluating the tracker. If you want to use these datasets, please follow their licenses, and if you use any of these datasets in your research, please cite the original work (you can find the BibTeX in the bottom). MOTChallenge and CrowdHuman are already downloaded on the remote server in Lab-304.
### 🥇 [MOTChallenge](https://motchallenge.net/)🔥🔥🔥
They have created a framework for the fair evaluation of multiple people tracking algorithms. In this framework we provide:

👉 A large collection of datasets, some already in use and some new challenging sequences!

👉 Detections for all the sequences.

👉 A common evaluation tool providing several measures, from recall to precision to running time.

👉 An easy way to compare the performance of state-of-the-art tracking methods.

👉 Several challenges with subsets of data for specific tasks such as 3D tracking, surveillance, sports analysis (updates coming soon).

Popular MOTDatasets [MOT15](https://motchallenge.net/data/MOT15/), [MOT16](https://motchallenge.net/data/MOT16/), [MOT17](https://motchallenge.net/data/MOT17/), [MOT20](https://motchallenge.net/data/MOT20/), and others can be download in [MOT Challenge](https://motchallenge.net/).

If you want to evaluate your models on MOTChallenge, you need to register in advance using the __Official Email__, as the review period for registration can take up to several months.

### 🥈 [CrowdHuman](https://www.crowdhuman.org/) 
[CrowdHuman](https://www.crowdhuman.org/) is a benchmark dataset to better evaluate detectors in crowd scenarios. The CrowdHuman dataset is large, rich-annotated and contains high diversity. [CrowdHuman](https://www.crowdhuman.org/) is often used for __additional training__ to enhance the detection capabilities of the tracking model.

### 🥉 Others
Other datasets can be find in [here](https://github.com/Zhongdao/Towards-Realtime-MOT/blob/master/DATASET_ZOO.md). 


